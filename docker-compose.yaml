version: "3.8"

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    volumes:
      - pgdata:/var/lib/postgresql/data

  airflow:
    image: apache/airflow:2.9.0-python3.11
    depends_on: [postgres]
    env_file: .env          # ← loads your secrets
    environment:
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
    ports:
      - "8080:8080"
    command: >
      bash -c "pip install pandas pyarrow boto3 requests python-dotenv snowflake-connector-python 'snowflake-sqlalchemy[pandas]' dbt-core dbt-snowflake && airflow db migrate && airflow users create --username admin --firstname A --lastname B --role Admin --email you@example.com --password admin && exec airflow webserver"

  airflow-scheduler:
    image: apache/airflow:2.9.0-python3.11
    depends_on: [airflow]
    env_file: .env
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
    command: >
      bash -c "pip install pandas pyarrow boto3 requests python-dotenv snowflake-connector-python 'snowflake-sqlalchemy[pandas]' dbt-core dbt-snowflake &&
               exec airflow scheduler"

volumes:
  pgdata:
